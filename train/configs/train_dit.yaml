# Defaults mirrored from train/train_dit.py
paths:
  root_dir: data/train
  hic_dirname: Hi-C
  struct_dirname: structure
  vae_ckpt: null  # auto -> checkpoints/vae/sdvae1dfinal.pt if compressed else vae1dfinal.pt
  save_dir: null  # auto -> checkpoints/dit/<model>-<size> when null

model:
  name: JointAttDiT  # choices: CrossDiT, JointAttDiT, MMDiTX
  size: L            # choices: S, B, L, XL
  use_seq_compression: true  # required for JointAttDiT and MMDiTX
  use_global_cond: false     # CrossDiT only
  grad_cp: true              # gradient checkpointing flag

training:
  epochs: 50
  batch_size: 8
  lr: 0.0001
  weight_decay: 0.0
  latent_scale: 1.335256
  sample_steps: 50
  warmup_steps: null  # auto -> 0 for CrossDiT else 1000 (enables cosine when >0)
  clip_grad_norm: 1.0
  num_workers: 4
  pin_memory: true
  precision: null  # auto -> fp32 for CrossDiT else fp16

inference_preview:
  batch_size: 10
  cfg_scale: null  # auto -> 1.0 for CrossDiT else 1.5

logging:
  run_name: rf_dit_structure
  wandb_project: rf_dit_structure
